{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oltur-NQ22jd",
        "outputId": "f4327700-4752-4a08-dc78-8deb62f3ed80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xjpA6N2C4aKn"
      },
      "outputs": [],
      "source": [
        "# train dataset\n",
        "\n",
        "\n",
        "class TrainLegibilityDataset(Dataset):\n",
        "    def __init__(self, txt_file, img_dir, transform=None, isBalanced=False):\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read CSV file\n",
        "        self.img_labels = pd.read_csv(txt_file, header=None, names=[\"image\", \"label\"])\n",
        "\n",
        "\n",
        "       # balancing the datatset to ensure thatlegible and illegible images are used in a balanced way to train the legibility classifier\n",
        "        if isBalanced:\n",
        "            legible = self.img_labels[self.img_labels[\"label\"] == 1]\n",
        "            count_legible = len(legible)\n",
        "            illegible = self.img_labels[self.img_labels[\"label\"] == 0]\n",
        "            # Sample from the larger class to match smaller class\n",
        "            if len(illegible) > count_legible:\n",
        "                illegible = illegible.sample(n=count_legible, random_state=42)\n",
        "            self.img_labels = pd.concat([legible, illegible])\n",
        "            print(f\"Balanced dataset: {count_legible} legible vs {len(self.img_labels) - count_legible} non-legible\")\n",
        "        else:\n",
        "            legible_count = (self.img_labels[\"label\"] == 1).sum()\n",
        "            total = len(self.img_labels)\n",
        "            print(f\"As-is dataset: {legible_count} legible out of {total} total samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.img_labels.iloc[idx]\n",
        "        img_filename = row[\"image\"]\n",
        "        label = row[\"label\"]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Return (image, label, filename)\n",
        "        return image, label, img_filename\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nfglfoEZZbv7"
      },
      "outputs": [],
      "source": [
        "\n",
        "#resnet\n",
        "class LegibilityClassifier34(nn.Module):\n",
        "    def __init__(self, finetune=False):\n",
        "\n",
        "        super(LegibilityClassifier34, self).__init__()\n",
        "        self.model_ft = models.resnet34(pretrained=True)\n",
        "        if finetune:\n",
        "            for param in self.model_ft.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        num_ftrs = self.model_ft.fc.in_features\n",
        "        self.model_ft.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 1),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        # We apply sigmoid in forward() to get probabilities\n",
        "    def forward(self, x):\n",
        "        x = self.model_ft(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_QAZnK7kkwOp"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs):\n",
        "\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # progress bar\n",
        "            phase_bar = tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Phase\", leave=False,dynamic_ncols=True)\n",
        "\n",
        "            for inputs, labels, _ in phase_bar:\n",
        "                # Convert labels to float, shape [batch_size, 1]\n",
        "                labels = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)  # shape: [batch_size, 1]\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Predictions: threshold at 0.5\n",
        "                    preds = (outputs >= 0.5).float()\n",
        "\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                phase_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print(f\"{phase} Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            # Save best weights\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "#evaluate\n",
        "\n",
        "def evaluate_model(model, dataloader, dataset_size):\n",
        "\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_filenames = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, filenames in tqdm(dataloader):\n",
        "            labels = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            preds = (outputs >= 0.5).float()\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            all_preds.extend(preds.cpu().numpy().flatten().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
        "            all_filenames.extend(filenames)\n",
        "\n",
        "    accuracy = running_corrects.double() / dataset_size\n",
        "    return accuracy.item(), all_preds, all_labels, all_filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqtx0at9mFaP",
        "outputId": "bd276540-82f6-4ef3-d22b-f9d9ebf55c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced dataset: 12422 legible vs 12408 non-legible\n",
            "As-is dataset: 1242 legible out of 2483 total samples.\n",
            "Number of training samples: 24830\n",
            "Number of validation samples: 2483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Phase:   0%|          | 0/6208 [00:00<?, ?it/s]<ipython-input-11-cec0a2ffae7c>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4396  Acc: 0.7243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.0587  Acc: 0.9774\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3684  Acc: 0.7444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.0615  Acc: 0.9932\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3647  Acc: 0.7451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.0867  Acc: 0.9940\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3625  Acc: 0.7480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.0252  Acc: 0.9948\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3558  Acc: 0.7488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.1198  Acc: 0.9895\n",
            "\n",
            "Training complete in 24m 5s\n",
            "Best val Acc: 0.9948\n",
            "Best model weights saved to: resnet34_legibility_soccernet_best.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data_root = \"/content/SoccerNetLegibility\"\n",
        "\n",
        "train_txt = os.path.join(data_root, \"train\", \"train_gt.txt\")\n",
        "train_img_dir = os.path.join(data_root, \"train\", \"images\")\n",
        "\n",
        "full_df = pd.read_csv(train_txt, header=None, names=[\"image\", \"label\"])\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=full_df[\"label\"]  # keeps class distribution similar\n",
        ")\n",
        "\n",
        "# Write these to temporary CSV files for our Dataset class\n",
        "train_df.to_csv(\"train_split.txt\", index=False, header=False)\n",
        "val_df.to_csv(\"val_split.txt\", index=False, header=False)\n",
        "\n",
        "\n",
        "# Transforms (for 'resnet')\n",
        "# We'll define them here, but you could also inline them in the dataset classes\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomGrayscale(),\n",
        "    transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = TrainLegibilityDataset(\n",
        "    txt_file=train_txt,\n",
        "    img_dir=train_img_dir,\n",
        "    transform=train_transform,\n",
        "    isBalanced=True  # If you want balanced classes\n",
        ")\n",
        "\n",
        "val_dataset = TrainLegibilityDataset(\n",
        "    txt_file=\"val_split.txt\",\n",
        "    img_dir=train_img_dir,\n",
        "    transform=val_transform,\n",
        "    isBalanced=False\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 4  # Adjust if you run into memory issues\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': train_loader,\n",
        "    'val': val_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    'train': len(train_dataset),\n",
        "    'val': len(val_dataset)\n",
        "}\n",
        "\n",
        "print(\"Number of training samples:\", dataset_sizes['train'])\n",
        "print(\"Number of validation samples:\", dataset_sizes['val'])\n",
        "\n",
        "# Instantiate the ResNet-34 model\n",
        "model = LegibilityClassifier34(finetune=False)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss, optimizer, and learning rate scheduler\n",
        "criterion = nn.BCELoss()  # since we have a final sigmoid in the model\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "model = train_model(model, dataloaders, dataset_sizes, criterion, optimizer, lr_scheduler, num_epochs)\n",
        "\n",
        "# Save the best model weights\n",
        "model_save_path = \"resnet34_legibility_soccernet_best.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Best model weights saved to: {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_wjwMfYOn1nv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}